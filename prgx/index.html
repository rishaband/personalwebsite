<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Project Page</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/css/website-styles/prgx.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin="anonymous" referrerpolicy="no-referrer" />

</head>
<body>

  <div class="container">
    <div class="top-nav">
      <div> <a style="color: white;" href="/day5/index.html">&larr; Back</a></div>
      <div><a href="/walker/index.html" style="color: white;">Next &rarr;</a></div>
    </div>

    <div class="title-section">
      <h1><em>PRGX Global</em></h1>
      <p>Data Science Intern</p>
    </div>

    <div class="main-image">
      <img src="/images/nicegxx.jpg" alt="Project Screenshot" />
    </div>

    <div class="info-grid">
      <div class="info-column">
        <h3>Timeline</h3>
        <ul>
          <li>Sep. 2024 – Dec. 2024</li>
          <li>CO-OP #2</li>
          <li>Intern Performance Rating: <a href="https://uwaterloo.ca/co-operative-education/your-work-term/work-term-evaluations" target="_blank" style="text-decoration: none; color: rgb(255, 255, 255);">Excellent <i class="fa-solid fa-arrow-up-right-from-square" style="color: #ffffff;"></i></a></li>
        </ul>

        <h3>Tech</h3>
        <ul>
          <li>Python</li>
          <li>SQL</li>
          <li>.Net</li>
          <li>VBA</li>
        </ul>
      </div>

      <div class="info-column overview">
        <h3>Overview</h3>
        <p>
          PRGX Global is a leading analytics-driven company specializing in recovery audit, spend analytics, and information management. With a global presence and a client base that includes many Fortune 500 companies, 
          PRGX helps organizations identify cost recovery opportunities. I joined the <a href="https://www.prgx.com/solutions/internal-audit/?_gl=1*emt9ll*_up*MQ..*_gs*MQ..&gclid=Cj0KCQjwoNzABhDbARIsALfY8VPwsazsWJrxiWDONrOD98fHvKTVbw1Evlukcc5Dt34oH-s6rnVDHoAaAoDeEALw_wcB&gbraid=0AAAAAqgbz1-quo1VMI94Di4czgL8avX4m" target="_blank" style="text-decoration: none; color: white; font-style: italic;">Audit Operations Team <i class="fa-solid fa-arrow-up-right-from-square" style="color: #ffffff;"></i></a>
          and contributed to the Data initiatives. 
        </p>
        <a href="https://www.prgx.com/" target="_blank" style="color: white; text-decoration: none;">Website <i class="fa-solid fa-arrow-up-right-from-square" style="color: #ffffff;"></i></a>
      </div>
    </div>

    <h3>Work Product</h3>
    <p>
      I focused on building data-driven infrastructure to streamline audit operations and enhance query efficiency. 
      I began with implementing & improving ETL pipelines to extract and transform data from large relational databases for further QA screening. 
      This included optimizing SQL queries and views. One of the core projects I worked on involved engineering a GUI-driven pipeline in Python to automate the 
      extraction and organization of embedded objects from complex Excel files. This tool drastically simplified internal audit processes. 
      To manage client datasets, I developed parallel column-level fluctuation detection scripts with the goal of identifying optimal segmentation criteria. 
      This enabled smart partitioning of combined tables and supported efficient deduplication logic.
      This naturally required system stored procedures so I also worked on optimizing SSP’s within the server environment. 
    </p>
    <h3>Internal Research</h3>
    <p>Conducted research into Random Forest anomaly detection approach for client-statistic report analysis. An internal tool would use SQL scripts to produce stat reports
      which would then be sent back to the client. However, prior to that, analytics must be done on the niche details of the report such as "vendor_name", "vendor_population", 
      "field_population", etc to make sure they match up with previous years. This was a hefty manual task that required hours to finish. It can be simplified by using historical validated reports. We
      would train our model to predict whether a row needs QA attention(binary prediction or confidence score). Admittedly - this approach is more complicated than say a <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html" target="_blank" style="text-decoration: none; color: white; font-style: italic; font-weight: bold;">One-Class SVM</a> as
      we are using a variant of Random Forest, called <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html" target="_blank" style="text-decoration: none; color: white; font-style: italic; font-weight: bold;">Isolation Forest</a>. 
      Overall, it was an insightful experience researching this approach and understanding it's relevance to 
      the data I was working with. 
    </p>
    <h3>Results</h3>
    <p>Saved on average ≈ <span style="font-weight: bold;">2.75 hours</span> per audit session</p>
    <p>Reduced QA screening time by <span style="font-weight: bold;">2x</span></p>
   
    <hr>

    <footer class="footer">
      <div class="footer-bar">
        <a href="/index.html" class="refbacklast" style="text-decoration: none;">Rishab Anand</a>
        <span class="right-status">
          <span class="pulse-wrapper">
              <i class="fa-solid fa-circle fa-xs" style="color: #11ff00;"></i>
          </span>
          Last updated 5/3/2025
        </span>
      </div>
    </footer>

  </div>

</body>
</html>
